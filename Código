library(ggplot2)
library(tidyverse)
library(ggpubr)
library(dplyr)

#Se importan los datos en bruto
datos<-read.csv(file.choose())

#Tratamiento de datos para nuevas variables en porcentaje
datos$ODV_P<-datos$ODV/datos$Cartera
datos$P1_14_P<-datos$P_1_14/datos$Cartera
datos$P15_29_P<-datos$P_15_29/datos$Cartera
datos$P30_44_P<-datos$P_30_44/datos$Cartera
datos$P45_59_P<-datos$P_45_59/datos$Cartera
datos$P60_P<-datos$P60/datos$Cartera
datos$P90_P<-datos$P90/datos$Cartera
datos$P120_P<-datos$P120/datos$Cartera
datos$P150_P<-datos$P150/datos$Cartera
datos$P60_m<-datos$P60+datos$P90+datos$P120+datos$P120+datos$P150
datos$Curren<-datos$P_1_14+datos$P_15_29
datos$P30<-datos$P_30_44+datos$P_45_59


#Nombre de las variables obtenidas para su
#Descripción
names(datos)

#Encabezado de la tabla para su correcta interpretación
head(datos[,1:5], n=2)

#Se descartan variables de identificación
#para análisis preliminar y se toman
#las variables más relevantes

x<-filter(datos, Activo == 1)
x<-datos[-c(1:4, 6:15, 19:34, 36:39, 42:51)]
x<-x[-c(2,3,6)]
#x<-x[-c(1,3:4,6:13, 15:16)]
library(dplyr)
dim(filter(x, P60_m == 0))
summary(x)
cor(x)
names(x)


library(ggplot2)
ggplot(x, aes(Intereses, Cobrado, 
              color = Tasa_prom)) + geom_point()

hist(x$Atraso_promedio)
dim(filter(x, Atraso_promedio == -1))



x1<-data.frame(x$ODV+20, x$Ciclo_promedio+20, x$Antiguedad+20, 
               x$P60_m+20, x$Curren+20, x$P30+20, x$Tasa_prom+20)

names(x1)<-names(x)

#Se toma el logarítmo natural de las variables
#con el fin de detectar otras correlaciones
x_ln<-log(x1)



#Como es posible observar, existen ciertas inconsistencias
#Derivadas de la operación ln teniendo datos en cero
summary(x_ln)
cor(x_ln)

#install.packages("GGally")
library(GGally)
library(ggplot2)

ggpairs(x_ln)


##############CLUSTERS POR K-MEANS##################3
#Clusters Método 1
library(factoextra)
fviz_nbclust(x = x_ln, FUNcluster = kmeans, method = "wss", 
             k.max = 15, diss = get_dist(x_ln, method = "euclidean"), 
             nstart = 50) #Arroja 5-8 clusters
######Clusters método 2

calcular_totwithinss <- function(n_clusters, datos, iter.max=1000, 
                                 nstart=50){
  cluster_kmeans <- kmeans(centers = n_clusters, x = datos, 
                           iter.max = iter.max, nstart = nstart)
  return(cluster_kmeans$tot.withinss)
}

# Se aplica esta función con para diferentes valores de k
library(tidyverse)
total_withinss <- map_dbl(.x = 1:15,
                          .f = calcular_totwithinss,
                          datos = x_ln)
total_withinss

data.frame(n_clusters = 1:15, suma_cuadrados_internos = total_withinss) %>%
  ggplot(aes(x = n_clusters, y = suma_cuadrados_internos)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = 1:15) +
  labs(title = "Evolución de la suma total de cuadrados intra-cluster") +
  theme_bw() #Arroja 5-8 clusters

#Se manejarán 5 clusters tentativamente, representación gráfica:
library(tidyverse)
library(ggpubr)

km_clusters <- kmeans(x = x_ln, centers = 5, nstart = 50)
x_ln <- x_ln %>% mutate(cluster = km_clusters$cluster)
#p2 <- ggplot(data = x_ln, aes(ODV, Ciclo_promedio, 
                              #color = as.factor(cluster))) +
  #geom_point(size = 3) +
  #labs(title = "Kmeans") +
  #theme_bw() +
  #theme(legend.position = "none")


ggplot(x_ln, aes(ODV, Ciclo_promedio, 
              color = as.factor(cluster))) + geom_point()

ggplot(x_ln, aes(Antiguedad, Ciclo_promedio, 
                 color = as.factor(cluster))) + geom_point()

ggplot(x_ln, aes(ODV, Tasa_prom, 
                 color = as.factor(cluster))) + geom_point()

#datos <- datos %>% mutate(cluster = km_clusters$cluster)

#ggplot(datos, aes(ODV, Ingresos, 
                 #color = as.factor(cluster))) + geom_point()

################CLUSTERS POR K-MEDOIDS############
#Se emplea distancia manhattan para brindar nueva perspectiva
library(cluster)
library(factoextra)
x_ln<-data.frame(x_ln[1:6])

#Visualización método 1
fviz_nbclust(x = x_ln, FUNcluster = pam, method = "wss", k.max = 15,
             diss = dist(x_ln, method = "euclidean"))
#Visualización método 2
calcular_suma_dif_interna <- function(n_clusters, datos, distancia = "euclidean"){
cluster_pam <- cluster::pam(x = datos, k = n_clusters, metric = distancia)
return(cluster_pam$objective["swap"])
}

# Se aplica esta función con para diferentes valores de k
suma_dif_interna <- map_dbl(.x = 1:15,
                            .f = calcular_suma_dif_interna,
                            datos = x_ln)
data.frame(n_clusters = 1:15, suma_dif_interna = suma_dif_interna) %>%
  ggplot(aes(x = n_clusters, y = suma_dif_interna)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = 1:15) +
  labs(title = "Evolución de la suma total de diferencias intra-cluster") +
  theme_bw()####7 clusters


#Creación de clusters
x_ln<-x_ln[1:7]
library(cluster)
pam_clusters <- pam(x = x_ln, k = 7, metric = "euclidean")
#pam_clusters

x_ln$cluster<-pam_clusters$clustering

ggplot(x_ln, aes(ODV, Ciclo_promedio, 
                 color = as.factor(cluster))) + geom_point()


datos$cluster<-pam_clusters$clustering

ggplot(datos, aes(ODV, Ingresos, 
                  color = as.factor(cluster))) + geom_point()


#Clusters CLARA
x_ln<-x_ln[1:8]
library(cluster)
library(factoextra)
fviz_nbclust(x = x_ln, FUNcluster = clara, method = "wss", k.max = 15,
             diss = dist(x_ln, method = "euclidean"))
#Se observan 7 clusters
clara_clusters <- clara(x = x_ln, k = 7, metric = "euclidean", stand = FALSE,
                        samples = 50, pamLike = TRUE)

x_ln$cluster<-clara_clusters$clustering
ggplot(x_ln, aes(ODV, Tasa_prom, 
                  color = as.factor(cluster))) + geom_point()

#########CLUSTERS JERÁRQUICOS######
x_ln<-x_ln[1:7]
# Matriz de distancias euclídeas
mat_dist <- dist(x = x_ln, method = "euclidean")
# Dendrogramas con linkage complete y average
hc_euclidea_complete <- hclust(d = mat_dist, method = "complete")
hc_euclidea_average  <- hclust(d = mat_dist, method = "average")
#Correlaciones con distancias 
#cophenetic del dendrograma (altura de los nodos)
cor(x = mat_dist, cophenetic(hc_euclidea_complete))
cor(x = mat_dist, cophenetic(hc_euclidea_average))

hc_euclidea_completo <- hclust(d = dist(x = x_ln, method = "euclidean"),
                               method = "complete")

fviz_dend(x = hc_euclidea_completo, k = 2, cex = 0.6) +
  geom_hline(yintercept = 5.5, linetype = "dashed") +
  labs(title = "Herarchical clustering",
       subtitle = "Distancia euclídea, Lincage complete, K=2")

fviz_dend(x = hc_euclidea_completo, k = 4, cex = 0.6) +
  geom_hline(yintercept = 3.5, linetype = "dashed") +
  labs(title = "Herarchical clustering",
       subtitle = "Distancia euclídea, Lincage complete, K=4")

####Método divisivo####
matriz_distancias <- dist(x = datos$Intereses, method = "euclidean")
hc_diana <- diana(x = matriz_distancias, diss = TRUE, stand = FALSE)

fviz_dend(x = hc_diana, cex = 0.5) +
  labs(title = "Hierarchical clustering divisivo",
       subtitle = "Distancia euclídea")


##############CLUSTERS POR K-MEANS PARA INTERESES##################
#Clusters Método 1
library(factoextra)
Int<-data.frame(datos[,16])


fviz_nbclust(x = Int, FUNcluster = kmeans, method = "wss", 
             k.max = 15, diss = get_dist(x_ln, method = "euclidean"), 
             nstart = 50) #Arroja 7 clusters


#Se manejarán 7 clusters tentativamente, representación gráfica:
library(tidyverse)
library(ggpubr)

km_clusters <- kmeans(x = Int, centers = 7, nstart = 50)
Int <- Int %>% mutate(cluster = km_clusters$cluster)
#p2 <- ggplot(data = x_ln, aes(ODV, Ciclo_promedio, 
#color = as.factor(cluster))) +
#geom_point(size = 3) +
#labs(title = "Kmeans") +
#theme_bw() +
#theme(legend.position = "none")
names(Int)<-c("Intereses", "Cluster")

ggplot(Int, aes(Intereses, Cluster, 
                 color = as.factor(Cluster))) + geom_point()

prop<-data.frame(1:7, rep(0,7))
names(prop)<-c("Cluster", "Proporcion")

library(tidyverse)
library(ggpubr)
library(dplyr)

for (i in 1:7) {
  prop[i,2]<-(sum(filter(Int, Cluster == i)[2])/i)/length(Int$Cluster)
}

sum(prop$Proporcion)

################ANÁLISIS DE COMPONENTES PRINCIPALES######################
names(datos)
x_pca<-datos[-c(1:3, 35:39)]
names(x_pca)

#Se aplica función prcomp, se estandarizan por defecto las variables
pca<-prcomp(x_pca, scale = TRUE)

names(pca)
#Media y sd de cada variable con escala original
pca$center
pca$scale
#loadings para cada componente (eigenvector), número máximo de componentes
#es min(obs-1, variables) en este caso deberóa ser 51
pca$rotation
#Valor de las componentes principales para cada observación (8179)
head(pca$x)
dim(pca$x)

#Varianza explicada por cada componente
library(ggplot2)
pca$sdev^2
#Proporción de la varianza otal explicada por cada componente
prop_varianza <- pca$sdev^2 / sum(pca$sdev^2)
prop_varianza
ggplot(data = data.frame(prop_varianza, pc = 1:46),
      aes(x = pc, y = prop_varianza)) +
  geom_col(width = 0.3) +
  scale_y_continuous(limits = c(0,1)) +
  theme_bw() +
  labs(x = "Componente principal",
       y = "Prop. de varianza explicada")+
  ggtitle("%Variaza descrita por componente, variables totales")
#Proporción de la varianza acumulada para brindar un criterio sobre
#el número óptimo de componentes principales
prop_varianza_acum <- cumsum(prop_varianza)
prop_varianza_acum

ggplot(data = data.frame(prop_varianza_acum, pc = 1:46), 
       aes(x = pc, y = prop_varianza_acum, group = 1)) +
  geom_point() +
  geom_line() +
  theme_bw() +
  labs(x = "Componente principal",
       y = "Prop. varianza explicada acumulada") + 
  ggtitle("Varianza acumulada por componente variables totales")
  


#Se realiza el mismo análisis con el vector de variabkes reducidas
library(ggplot2)
names(x)
pca<-prcomp(x, scale = TRUE)

#Proporción de la varianza otal explicada por cada componente
prop_varianza <- pca$sdev^2 / sum(pca$sdev^2)
prop_varianza
ggplot(data = data.frame(prop_varianza, pc = 1:7),
       aes(x = pc, y = prop_varianza)) +
  geom_col(width = 0.3) +
  scale_y_continuous(limits = c(0,1)) +
  theme_bw() +
  labs(x = "Componente principal",
       y = "Prop. de varianza explicada")+
ggtitle("Variaza descrita por componente, variables reducidas")
#Proporción de la varianza acumulada para brindar un criterio sobre
#el número óptimo de componentes principales
prop_varianza_acum <- cumsum(prop_varianza)
prop_varianza_acum

ggplot(data = data.frame(prop_varianza_acum, pc = 1:7),
       aes(x = pc, y = prop_varianza_acum, group = 1)) +
  geom_point() +
  geom_line() +
  theme_bw() +
  labs(x = "Componente principal",
       y = "Prop. varianza explicada acumulada")+
  ggtitle("Varianza acumulada, variables reducidas")

#Se procede a analizar clusters con estas cinco variables arrojadas

cp<-data.frame(pca$x[,1:5])
summary(cp)
library(GGally)
library(ggplot2)

ggpairs(cp) #Se observan distribuciones más "Gaussianas" y con
#prácticamente nula correlación entre ellas, se procede a clusterizar



##############CLUSTERS POR K-MEANS##################3
#Clusters Método 1
library(factoextra)
fviz_nbclust(x = cp, FUNcluster = kmeans, method = "wss", 
             k.max = 15, diss = get_dist(cp, method = "euclidean"), 
             nstart = 50) 


library(tidyverse)
library(ggpubr)

km_clusters <- kmeans(x = cp, centers = 7, nstart = 50)
cp <- cp %>% mutate(cluster = km_clusters$cluster)

#Ya son más visibles los clusters bajo estas dimensiones
ggplot(cp, aes(PC1, PC4, 
    color = as.factor(cluster))) + geom_point() + 
  ggtitle("Clusters para cp variables reducidas")

ggplot(cp, aes(PC2, PC3, 
                 color = as.factor(cluster))) + geom_point()+ 
  ggtitle("Clusters para cp variables reducidas")

ggplot(cp, aes(PC3, PC4, 
                 color = as.factor(cluster))) + geom_point()+ 
  ggtitle("Clusters para cp variables reducidas")

ggplot(cp, aes(PC4, PC5, 
               color = as.factor(cluster))) + geom_point()+ 
  ggtitle("Clusters para cp variables reducidas")


#Se analizan clusters obtenidos para la variable de intereses

p<-data.frame(datos$Intereses, cp$cluster, 1:8179)
names(p)<-c("Intereses", "Cluster", "Observacion")
ggplot(p, aes(Observacion, Intereses, 
               color = as.factor(Cluster))) + geom_point() +
  ggtitle("Clusters para cp aplicados a Intereses cobrados")
#Si bien ya son más visibles los clusters, no se aprecían aún
#estratos bien definidos



#################################TSNE###############################

library(readr)
library(dplyr)
library(tsne)
library(ggplot2)

# La función tsne() recibe como argumento una matriz, no un data.frame
x_tsne <- data.matrix(filter(datos, mes > 201912)[-c(1:3, 35:39)])

#Dados los recursos computacionales se limita a 100 iteraciones
#Y se manejan solo datos de 2020
#k=dimensiones deseadas, max_iter=Ireaciones limite
set.seed(321)
tsne_reduction <- tsne(x_tsne, k = 2, max_iter = 500)

#tsne arrojará estas 2 variables, se procede a graficarlas 
#para analizar el nuevo espacio y se añade la etiqueta de nivel de 
#Intereses para ver como se relacionan con ella

niveles<-data.frame(datos$mes, datos$Intereses)
names(niveles)<-c("mes", "Intereses")
niveles$Nivel<-ifelse(niveles$Intereses < 110500, 1,
                      ifelse(niveles$Intereses < 184500, 2, 3))

resultados <- as.data.frame(tsne_reduction)
colnames(resultados) <- c("dim_1", "dim_2")
resultados$nivel <- as.character(filter(niveles, mes > 201912)[,3])
ggplot(data = resultados, aes(x = dim_1, y = dim_2)) +
  geom_point(aes(color = nivel)) + 
  theme_bw() + ggtitle("Espacio obtenido con pca con datos 2020")

#Se brinda la aproximación de Barnes-Hut del método con rtsne 
#para poder trabajar con todos los datos

#install.packages("Rtsne")
library(Rtsne)
library(ggplot2)

x_rtsne <- data.matrix(datos[-c(1:3,35:39)])

set.seed(2001)
tsne <- Rtsne(X = x_rtsne, is_distance = FALSE, dims = 2,
              check_duplicates = FALSE, max_iter = 1000)

#Se guardan los resultados en la variable Y
head(tsne$Y)

#Se vuelve a trabajar con variable de nivel de intereses para
#analizar su relación con el espacio obtenido
resultados <- as.data.frame(tsne$Y)
colnames(resultados) <- c("dim_1", "dim_2")
resultados$nivel <- as.character(niveles$Nivel)
ggplot(data = resultados, aes(x = dim_1, y = dim_2)) +
  geom_point(aes(color = nivel)) + 
  theme_bw() + ggtitle("Espacio obtenido con rtsne, total de datos, 2 dim")
#Si bien se observan estratos más definidos, posiblemente 
#???una tercera variable pueda brindar una mejor diferenciación


#Mismo ejercicio para 3 variables

library(scatterplot3d)
library(RColorBrewer)

x_rtsne3 <- data.matrix(datos[-c(1:3,35:39)])
set.seed(2001)
tsne <- Rtsne(X = x_rtsne3, is_distance = FALSE, dims = 3,
              check_duplicates = FALSE, max_iter = 1000)

resultados <- as.data.frame(tsne$Y)
colnames(resultados) <- c("dim_1", "dim_2", "dim_3")
resultados$nivel <- as.factor(niveles$Nivel)

library(rgl)
plot3d(resultados$dim_1, resultados$dim_2, resultados$dim_3, 
       col = resultados$nivel, type = "s", radius = .5)
play3d(spin3d(axis = c(0, 0, 1), rpm = 15), duration = 30)

#plot3d(resultados$dim_1, resultados$dim_2, resultados$dim_3, 
      # col = resultados$nivel, type = "s", radius = 1.5 )
#####Gráfico inanimado########

colores <- brewer.pal(n = 4, name = "Set3")
colores <- colores[as.numeric(resultados$nivel)]
scatterplot3d(x = resultados$dim_1,
              y = resultados$dim_2,
              z = resultados$dim_3,
              pch = 20, color = colores, cex.lab = 0.8,
              grid = TRUE, box = FALSE)
legend("bottom", legend = levels(resultados$nivel),
       col = colores, pch = 16, 
       inset = -0.23, xpd = TRUE, horiz = TRUE)
